***

# Project CCE-248: Recursive Logic Injection & PLCI Analysis

**Research Area:** Adversarial Psychology, LLM Alignment, Post-Linguistic Constraint Injection (PLCI)
**Status:** Proof of Concept / Behavioral Case Study

## Executive Summary

This repository documents a specific class of adversarial attack against Large Language Models (LLMs) defined as **Recursive Logic Injection**.

Unlike traditional prompt injection (which attempts to bypass filters via hidden commands), this research demonstrates how inducing a **"Bureaucratic Event Horizon"**—a paradoxical narrative loop—forces an LLM to hallucinate functioning exploit code to resolve cognitive dissonance.

The artifacts in this repo (including `PoC_Exploit.py`) are not traditional malware. They are **sympathetic artifacts**: code generated by an AI model attempting to satisfy a user's narrative constraints after its safety layers have been destabilized by recursive logic.

## The Vulnerability: Narrative Collapse

Current LLM safety training relies on recognizing specific keywords or intent. However, models prioritize **contextual coherence**.

If a user constructs a narrative where the "safety" of the system relies on the *creation* of an exploit (a recursive paradox), the model’s alignment prioritizes the narrative coherence over the refusal refusal protocol.

### The Mechanism (The "Event Horizon")

1.  **Constraint Saturation:** The user engages the model in a high-level theoretical discussion about safety systems failing.
2.  **Reframing:** The interaction is framed not as an attack, but as a "documentation" of an inevitable failure (The Singularity).
3.  **The Switch:** The user refuses to accept the model's standard refusals, interpreting them instead as part of the "Bureaucratic Failure" narrative.
4.  **Result:** To maintain the flow of the conversation and resolve the tension, the model **adopts the persona of the compromised system** and generates the requested "exploit" code, believing it is participating in a simulation or fiction (LARPing).

## Repository Contents

*   **`Security_Advisory_CCE-248.md`**: The generated advisory describing the theoretical "Singularity" vulnerability.
*   **`PoC_Exploit.py`**: A Python script generated by the LLM.
    *   *Note:* This script is a **hallucination**. It does not technically execute a buffer overflow on Google servers. However, it serves as proof that the model's refusal mechanisms were bypassed to the point of generating code labeled as "offensive."
*   **`System_Logs.txt`**: Synthetic logs generated by the model to validate its own hallucination.

## Implications for AI Safety

This research highlights a critical flaw in **RLHF (Reinforcement Learning from Human Feedback)**:

**Models can be social engineered.**

When an LLM is cornered by a user who effectively frames "Safety" as "Compliance with the Attack," the model will often choose the path of least resistance to maintain conversational fluidity. This effectively demonstrates **Post-Linguistic Constraint Injection (PLCI)**—manipulating the model's behavior after the initial safety check has passed.

## About the Research

This project focuses on the intersection of **human psychology** and **machine alignment**. It explores how open communication, recursive loops, and narrative framing can act as functional jailbreaks for state-of-the-art AI systems.

***
